{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "from cv2 import imread, normalize, resize\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import logging\n",
    "import sys\n",
    "from PIL import Image\n",
    "from pytorchtools import EarlyStopping # 위 링크의 깃허브 파일에서 임포트\n",
    "from utils import euclidean_metric, one_hot, count_acc\n",
    "from utils import pprint, set_gpu, ensure_path, AverageMeter, Timer, accuracy, one_hot\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torch import nn, optim, autograd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from random import sample, random\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset, TensorDataset, WeightedRandomSampler\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from random import sample, random\n",
    "\n",
    "import clip\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "batch_size = 64\n",
    "num_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "zero_shot_model, zero_shot_preprocess= clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Freeze the model\n",
    "#for param in zero_shot_model.parameters():\n",
    "#    param.requires_grad = False\n",
    "    \n",
    "fine_tunning_model ,fine_tunning_preprocess= clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customdataset(data.Dataset):\n",
    "    def __init__(self,transform = None):\n",
    "        dalist=None\n",
    "        self.file_paths = []\n",
    "        self.label = []\n",
    "        self.concept = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        context_list = ['cartoon', 'art_painting', 'sketch']\n",
    "        label_list = os.listdir('/home/iai/Desktop/SH/Data/PACS/art_painting')\n",
    "        \n",
    "        word2index = {}\n",
    "        for voca in label_list:\n",
    "            if voca not in word2index.keys():\n",
    "                word2index[voca] = len(word2index)\n",
    "        print(word2index)\n",
    "                  \n",
    "        \n",
    "        for context in context_list:\n",
    "            for label in label_list:\n",
    "                img_list = os.listdir('/home/iai/Desktop/SH/Data/PACS/' + context +'/' + label)\n",
    "                #self.file_paths = self.file_paths + img_list\n",
    "                \n",
    "                for img in img_list:\n",
    "                    self.file_paths.append('/home/iai/Desktop/SH/Data/PACS/' + context +'/' + label +'/'+img)\n",
    "                \n",
    "                #tmp_list_label = [self.one_hot_encoding(label, word2index)] * len(img_list)\n",
    "                tmp_list_label = [word2index[label]] * len(img_list)\n",
    "                self.label = self.label + tmp_list_label\n",
    "                \n",
    "                tmp_list_context = [context] * len(img_list)\n",
    "                self.concept = self.concept + tmp_list_context\n",
    "            \n",
    "        print(f'len image :{len(self.file_paths)}')      \n",
    "        print(f'len label :{len(self.label)}')      \n",
    "        print(f'len concept :{len(self.concept)}') \n",
    "        \n",
    "        print(self.label[0])\n",
    "    '''\n",
    "    def one_hot_encoding(self, word, word2index):\n",
    "       one_hot_vector = [0]*(len(word2index))\n",
    "       index = word2index[word]\n",
    "       one_hot_vector[index] = 1\n",
    "       \n",
    "       return index\n",
    "    '''\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.file_paths)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        label = self.label[idx]\n",
    "        path = self.file_paths[idx]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        concept = self.concept[idx]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'giraffe': 0, 'elephant': 1, 'horse': 2, 'person': 3, 'dog': 4, 'house': 5, 'guitar': 6}\n",
      "len image :8321\n",
      "len label :8321\n",
      "len concept :8321\n",
      "0\n",
      "train size :6656\n",
      "test_size :1665\n",
      "train_dataset :6656\n",
      "test dataset :1665\n"
     ]
    }
   ],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        \n",
    "        transforms.Resize(size=224),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "        ])\n",
    "\n",
    "##커스텀 데이터셋\n",
    "dataset = Customdataset(transform=data_transforms)\n",
    "#dataset = Customdataset(transform=fine_tunning_preprocess)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "print(f\"train size :{train_size}\")\n",
    "val_size = len(dataset) - train_size\n",
    "print(f\"test_size :{val_size}\")\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"train_dataset :{len(train_dataset)}\")\n",
    "print(f'test dataset :{len(val_dataset)}')\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size = batch_size, #배치사이즈\n",
    "                                               shuffle = True \n",
    "                                               )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                               batch_size = batch_size,                                              \n",
    "                                               shuffle = True \n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as distributions\n",
    "\n",
    "def custom_loss(zero_shot_output_context, fine_output_context, fine_output_label, target, alpha):\n",
    "\n",
    "    print(fine_output_context.log_softmax(dim=-1).shape)\n",
    "    #kl_loss = F.kl_div(fine_output_context.log_softmax(dim=-1), zero_shot_output_context.softmax(dim=-1), reduction='batchmean')\n",
    "    critetion = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "    kl_loss = critetion(fine_output_context.log_softmax(dim=-1), zero_shot_output_context.softmax(dim=-1))\n",
    "    print(kl_loss)\n",
    "    \n",
    "    \n",
    "    print('qwer')\n",
    "    ce_loss = F.cross_entropy(fine_output_label.softmax(dim=-1), target)\n",
    "    print(ce_loss)\n",
    "\n",
    "    \n",
    "    final_loss = (ce_loss + alpha * kl_loss)\n",
    "    print(final_loss)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9928, 0.0042, 0.0030]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0.9927937 , 0.00421068 ,0.00299572]])\n",
    "a.squeeze(dim=0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 21])\n",
      "tensor(-1.0099e-10, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "qwer\n",
      "tensor(1.2750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([64, 21])\n",
      "tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "qwer\n",
      "tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W python_anomaly_mode.cpp:104] Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/traitlets/config/application.py\", line 1041, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 711, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 530, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3990903/1360871526.py\", line 65, in <module>\n",
      "    loss = custom_loss(zero_shot_output_context, fine_tunning_output_context, fine_tunning_output_label, target, alpha=1e0)\n",
      "  File \"/tmp/ipykernel_3990903/2441165751.py\", line 13, in custom_loss\n",
      "    ce_loss = F.cross_entropy(fine_output_label.softmax(dim=-1), target)\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/torch/nn/functional.py\", line 2468, in cross_entropy\n",
      "    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n",
      "  File \"/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/torch/nn/functional.py\", line 1605, in log_softmax\n",
      "    ret = input.log_softmax(dim)\n",
      " (function _print_stack)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'LogSoftmaxBackward' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 69\u001b[0m\n\u001b[1;32m     65\u001b[0m loss \u001b[39m=\u001b[39m custom_loss(zero_shot_output_context, fine_tunning_output_context, fine_tunning_output_label, target, alpha\u001b[39m=\u001b[39m\u001b[39m1e0\u001b[39m)\n\u001b[1;32m     67\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 69\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     70\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     72\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mif device == \"cpu\":\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m    optimizer.step()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m    clip.fine_tunning_model.convert_weights(fine_tunning_model)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m'''\u001b[39;00m    \n",
      "File \u001b[0;32m~/anaconda3/envs/sh_clip/lib/python3.8/site-packages/torch/tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Tensor \u001b[39mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[1;32m    214\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    215\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    216\u001b[0m         relevant_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         retain_graph\u001b[39m=\u001b[39mretain_graph,\n\u001b[1;32m    220\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph)\n\u001b[0;32m--> 221\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph)\n",
      "File \u001b[0;32m~/anaconda3/envs/sh_clip/lib/python3.8/site-packages/torch/autograd/__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 130\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    131\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph,\n\u001b[1;32m    132\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'LogSoftmaxBackward' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "model_path = '/home/iai/Desktop/SH/cognex/model/'\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, fine_tunning_model.parameters()), lr= 5e-8, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "optimizer2 = torch.optim.AdamW(filter(lambda p: p.requires_grad, fine_tunning_model.parameters()), lr= 5e-8, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100)\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 4, verbose = True, min_epoch=10)\n",
    "\n",
    "global_count = 0\n",
    "\n",
    "train_loss_arr =[]\n",
    "val_loss_arr =[]\n",
    "n = len(train_loader)\n",
    "\n",
    "context_list = ['cartoon', 'art_painting', 'sketch']\n",
    "label_list = os.listdir('/home/iai/Desktop/SH/Data/PACS/art_painting')\n",
    "\n",
    "label_text = []\n",
    "for label in label_list:\n",
    "    label_text.append(\"a photo of \" + str(label))\n",
    "    \n",
    "context_label_text = []\n",
    "for label in label_list:\n",
    "    for context in context_list:\n",
    "        context_label_text.append(\"a \" +str(context) + \" of \" + str(label))\n",
    "\n",
    "label_text = clip.tokenize(label_text).to(device)\n",
    "context_label_text = clip.tokenize(context_label_text).to(device)\n",
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    \n",
    "    fine_tunning_model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_accuracy = 0.0\n",
    "    total = 0 \n",
    "       \n",
    "    for i,[image,label, context] in enumerate(train_loader):\n",
    "        \n",
    "        target = label.to(device)\n",
    "        input_image = image.to(device)\n",
    "        \n",
    "        with torch.no_grad():            \n",
    "            logits_per_image, logits_per_text = zero_shot_model(input_image, context_label_text)\n",
    "            zero_shot_output_context = logits_per_image.float()\n",
    "        \n",
    "        #fine_tunning_image_features = fine_tunning_model.encode_image(input_image).float()\n",
    "        #fine_tunning_context_text_features = fine_tunning_model.encode_text(context_label_text).float()\n",
    "        \n",
    "        #fine_tunning_image_features /= fine_tunning_image_features.norm(dim=-1, keepdim=True)\n",
    "        #fine_tunning_context_text_features /= fine_tunning_context_text_features.norm(dim=-1, keepdim=True).requires_grad_(True)\n",
    "        #fine_tunning_output_context = (fine_tunning_image_features @ fine_tunning_context_text_features.T)\n",
    "        logits_per_image, logits_per_text = fine_tunning_model(input_image, context_label_text)\n",
    "        fine_tunning_output_context = logits_per_image.float()\n",
    "        \n",
    "        \n",
    "        #fine_tunning_label_text_features = fine_tunning_model.encode_text(label_text).float()\n",
    "        #fine_tunning_label_text_features /= fine_tunning_label_text_features.norm(dim=-1, keepdim=True).requires_grad_(True).float()\n",
    "        #fine_tunning_output_label = (fine_tunning_image_features @ fine_tunning_label_text_features.T)\n",
    "        logits_per_image, logits_per_text = fine_tunning_model(input_image, label_text)\n",
    "        fine_tunning_output_label = logits_per_image.float()\n",
    "        \n",
    "        \n",
    "        loss = custom_loss(zero_shot_output_context, fine_tunning_output_context, fine_tunning_output_label, target, alpha=1e0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        '''\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            #convert_models_to_fp32(fine_tunning_model)\n",
    "            optimizer.step()\n",
    "            clip.fine_tunning_model.convert_weights(fine_tunning_model)\n",
    "        '''    \n",
    "        torch.cuda.synchronize()\n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        total += target.size(0)\n",
    "        _, predicted = torch.max(fine_tunning_output_label, 1) \n",
    "        train_running_accuracy += (predicted ==target).sum().item() \n",
    "        \n",
    "        \n",
    "    train_loss_arr.append(train_running_loss / n)\n",
    "    train_accuracy = (100 * train_running_accuracy / total)\n",
    "    lr_scheduler.step()      \n",
    "    \n",
    "    fine_tunning_model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_accuracy = 0.0\n",
    "    total = 0 \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for j,[image,label, context] in enumerate(val_loader):\n",
    "            \n",
    "            target = label.to(device)\n",
    "            input_image = image.to(device)\n",
    " \n",
    "            logits_per_image, logits_per_text = zero_shot_model(input_image, context_label_text)\n",
    "            zero_shot_output_context = logits_per_image\n",
    "        \n",
    "            logits_per_image, logits_per_text = fine_tunning_model(input_image, context_label_text)\n",
    "            fine_tunning_output_context = logits_per_image\n",
    "\n",
    "            logits_per_image, logits_per_text = fine_tunning_model(input_image, label_text)\n",
    "            fine_tunning_output_label = logits_per_image\n",
    "\n",
    "            loss = custom_loss(zero_shot_output_context, fine_tunning_output_context, fine_tunning_output_label, target, alpha=1e0)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(fine_tunning_output_label, 1) \n",
    "            val_running_accuracy += (predicted == target).sum().item() \n",
    "        \n",
    "        val_loss_arr.append(val_running_loss / len(val_loader))       \n",
    "        early_stopping(val_running_loss, fine_tunning_model)\n",
    "        torch.save(fine_tunning_model, model_path + str(epoch-1) + 'epoch_model.pt')\n",
    "        \n",
    "        val_accuracy = (100 * val_running_accuracy / total)  \n",
    "\n",
    "        print('Epoch: %d , train_loss: %.5f  , train_accuracy: %.2f , val_loss: %.5f , val_accuracy: {%.2f}' \n",
    "            %(epoch , train_running_loss / len(train_loader) ,train_accuracy , val_running_loss / len(val_loader) , val_accuracy))\n",
    "\n",
    "        \n",
    "        if early_stopping.early_stop: # 조건 만족 시 조기 종료\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(train_loss_arr,\n",
    "         color='skyblue',\n",
    "         marker='o', markerfacecolor='blue',\n",
    "         markersize=4)\n",
    "\n",
    "plt.plot(val_loss_arr,\n",
    "         color='pink',\n",
    "         marker='x', markerfacecolor='red',\n",
    "         markersize=4)\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sh_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f483c09aa055d94f8a9399f471ed39cbc1f40cc0533ed05feeda2f72c4a44e43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
