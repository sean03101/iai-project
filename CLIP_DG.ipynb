{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iai/anaconda3/envs/sh_clip/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "from cv2 import imread, normalize, resize\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import logging\n",
    "import sys\n",
    "from PIL import Image\n",
    "from pytorchtools import EarlyStopping # 위 링크의 깃허브 파일에서 임포트\n",
    "from utils import euclidean_metric, one_hot, count_acc\n",
    "from utils import pprint, set_gpu, ensure_path, AverageMeter, Timer, accuracy, one_hot\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torch import nn, optim, autograd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from random import sample, random\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset, TensorDataset, WeightedRandomSampler\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from random import sample, random\n",
    "\n",
    "import clip\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "batch_size = 32\n",
    "num_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "zero_shot_model, zero_shot_preprocess= clip.load(\"RN50\", device=device)\n",
    "\n",
    "# Freeze the model\n",
    "for param in zero_shot_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "fine_tunning_model ,fine_tunning_preprocess= clip.load(\"RN50\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customdataset(data.Dataset):\n",
    "    def __init__(self,transform = None):\n",
    "        dalist=None\n",
    "        self.file_paths = []\n",
    "        self.label = []\n",
    "        self.concept = []\n",
    "        self.transform = transform\n",
    "\n",
    "        folder_list = [\"custom_blur\",\"custom_brightness\",\"custom_cameraz\",\"custom_lightness\"]\n",
    "        \n",
    "        for folder_name in folder_list:\n",
    "            path_dir = \"/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/augmented_image2/train1-1/\" +  folder_name\n",
    "            file_list = os.listdir(path_dir)\n",
    "            \n",
    "            for file_name in file_list:\n",
    "                self.file_paths.append(path_dir + '/' + file_name)\n",
    "                self.label.append(file_name.split('_')[-1][0])\n",
    "                \n",
    "                if str(file_name.split('_')[2]) == 'cameraz':\n",
    "                    self.concept.append(3)\n",
    "                    \n",
    "                elif str(file_name.split('_')[2]) == 'blur':\n",
    "                    self.concept.append(3)\n",
    "                    \n",
    "                elif str(file_name.split('_')[2]) == 'brigntness':\n",
    "                    self.concept.append(2)\n",
    "                    \n",
    "                    \n",
    "                elif str(file_name.split('_')[2]) == 'lightness':\n",
    "                    self.concept.append(1)\n",
    "\n",
    "        \n",
    "        \n",
    "        file_path = '/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/imageset/single_image.2class.subject-split.trainval-repeat0/fold.5-5/ratio/100%/train.1-1.txt' # 폴더 경로\n",
    "\n",
    "        with open(file_path) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        image_names = [line.rstrip('\\n') for line in lines]\n",
    "        \n",
    "                \n",
    "        with open('/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/annotation/single_image.6class.json', 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            \n",
    "        path_image = \"/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/image/\"\n",
    "        files = os.listdir(path_image)\n",
    "        \n",
    "        for file in files:\n",
    "            if file in image_names:\n",
    "                self.label.append(data['single_image'][file]['class'][0])\n",
    "                self.file_paths.append(path_image + '/' + file)\n",
    "                self.concept.append(0)\n",
    "        \n",
    "        \n",
    "                \n",
    "        file_path = '/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/imageset/single_image.2class.subject-split.trainval-repeat0/fold.5-5/ratio/100%/validation.1-1.txt' # 폴더 경로\n",
    "\n",
    "        for folder_name in folder_list:\n",
    "            path_dir = \"/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/augmented_image2/train1-1/\" +  folder_name\n",
    "            file_list = os.listdir(path_dir)\n",
    "            \n",
    "            for file_name in file_list:\n",
    "                self.file_paths.append(path_dir + '/' + file_name)\n",
    "                self.label.append(file_name.split('_')[-1][0])\n",
    "                \n",
    "                if str(file_name.split('_')[2]) == 'cameraz':\n",
    "                    self.concept.append(3)\n",
    "                    \n",
    "                elif str(file_name.split('_')[2]) == 'blur':\n",
    "                    self.concept.append(3)\n",
    "                    \n",
    "                elif str(file_name.split('_')[2]) == 'brigntness':\n",
    "                    self.concept.append(2)\n",
    "                    \n",
    "                    \n",
    "                elif str(file_name.split('_')[2]) == 'lightness':\n",
    "                    self.concept.append(1)\n",
    "        \n",
    "        with open(file_path) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        image_names = [line.rstrip('\\n') for line in lines]\n",
    "        \n",
    "                \n",
    "        with open('/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/annotation/single_image.6class.json', 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            \n",
    "        path_image = \"/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/image/\"\n",
    "        files = os.listdir(path_image)\n",
    "        \n",
    "        for file in files:\n",
    "            if file in image_names:\n",
    "                self.label.append(data['single_image'][file]['class'][0])\n",
    "                self.file_paths.append(path_image + '/' + file)\n",
    "                self.concept.append(0) \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        label = self.label[idx]\n",
    "        idx_num = idx\n",
    "        tmp_prob = np.random.rand()\n",
    "        weight_dict = {\"0\" :  (400/250) / (400/250 + 400 * 5 / 30), \"1\" : (400/30) / (400/250 + 400 * 5 / 30), \"2\" : (400/30) / (400/250 + 400 * 5 / 30), \n",
    "                       \"3\" : (400/30) / (400/250 + 400 * 5 / 30), \"4\" : (400/30) / (400/250 + 400 * 5 / 30), \"5\" : (400/30) / (400/250 + 400 * 5 / 30)}        \n",
    "        \n",
    "        if weight_dict[str(label)] > tmp_prob:\n",
    "            while True : \n",
    "                idx_num = np.random.randint(len(self.file_paths))\n",
    "                tmp_prob =  np.random.rand()\n",
    "                label = self.label[idx_num]\n",
    "                \n",
    "                if weight_dict[str(label)] < tmp_prob:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        path = self.file_paths[idx_num]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        image = img.crop((30,48,482,235))\n",
    "        concept = self.concept[idx_num]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, int(label), int(concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size :8576\n",
      "test_size :2144\n",
      "train_dataset :8576\n",
      "test dataset :2144\n"
     ]
    }
   ],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        \n",
    "        transforms.Resize(size=224),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "        ])\n",
    "\n",
    "##커스텀 데이터셋\n",
    "dataset = Customdataset(transform=data_transforms)\n",
    "#dataset = Customdataset(transform=fine_tunning_preprocess)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "print(f\"train size :{train_size}\")\n",
    "val_size = len(dataset) - train_size\n",
    "print(f\"test_size :{val_size}\")\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"train_dataset :{len(train_dataset)}\")\n",
    "print(f'test dataset :{len(val_dataset)}')\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size = batch_size, #배치사이즈\n",
    "                                               shuffle = True \n",
    "                                               )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                               batch_size = batch_size,                                              \n",
    "                                               shuffle = True \n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel, preprocess = clip.load(\"RN50\", device=device)\\nmodel.cuda().eval()\\ninput_resolution = model.visual.input_resolution\\ncontext_length = model.context_length\\nvocab_size = model.vocab_size\\n\\nprint(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\\nprint(\"Input resolution:\", input_resolution)\\nprint(\"Context length:\", context_length)\\nprint(\"Vocab size:\", vocab_size)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "model, preprocess = clip.load(\"RN50\", device=device)\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=PIL.Image.BICUBIC)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7f1f7a8e2c10>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as distributions\n",
    "\n",
    "def custom_loss(zero_shot_output_context, fine_output_context, fine_output_label, target, alpha):\n",
    "\n",
    "    kl_loss = F.kl_div(zero_shot_output_context, fine_output_context, reduction = 'batchmean').requires_grad_(True)\n",
    "    ce_loss = F.cross_entropy(fine_output_label, target)\n",
    "    \n",
    "    final_loss = (ce_loss + alpha * kl_loss)\n",
    "    #print(final_loss)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 6.0000],\n",
      "        [7.5000, 9.0000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.0, 2.0],[3.0, 4.0]], requires_grad = True)\n",
    "x2 =  torch.tensor([[1.0, 2.0],[3.0, 4.0]], requires_grad = True)\n",
    "# tensor([[1., 2.],\n",
    "#         [3., 4.]], requires_grad=True)\n",
    "\n",
    "y = x + 2\n",
    "# tensor([[3., 4.],\n",
    "#         [5., 6.]], grad_fn=<AddBackward0>)\n",
    "\n",
    "z = y * y * 3\n",
    "# tensor([[ 27.,  48.],\n",
    "#         [ 75., 108.]], grad_fn=<MulBackward0>)\n",
    "\n",
    "out = z.mean()\n",
    "# tensor(64.5000, grad_fn=<MeanBackward0>)\n",
    "#out = F.kl_div(x, x2)\n",
    "\n",
    "out.backward()\n",
    "print(x.grad)\n",
    "# tensor([[4.5000, 6.0000],\n",
    "#         [7.5000, 9.0000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 120.047886).  Saving model ...\n",
      "Epoch: 1 , train_loss: 0.04127  , train_accuracy: 51.70 , val_loss: 1.79176 , val_accuracy: {64.65}\n",
      "Validation loss decreased (120.047886 --> 120.047886).  Saving model ...\n",
      "Epoch: 2 , train_loss: 0.07449  , train_accuracy: 66.32 , val_loss: 1.79176 , val_accuracy: {64.88}\n",
      "Validation loss decreased (120.047886 --> 120.047886).  Saving model ...\n",
      "Epoch: 3 , train_loss: 0.07449  , train_accuracy: 66.30 , val_loss: 1.79176 , val_accuracy: {65.25}\n",
      "Validation loss decreased (120.047886 --> 120.047886).  Saving model ...\n",
      "Epoch: 4 , train_loss: 0.07449  , train_accuracy: 66.03 , val_loss: 1.79176 , val_accuracy: {64.51}\n",
      "Validation loss decreased (120.047886 --> 120.047886).  Saving model ...\n",
      "Epoch: 5 , train_loss: 0.07449  , train_accuracy: 66.41 , val_loss: 1.79176 , val_accuracy: {64.83}\n",
      "Validation loss decreased (120.047886 --> 120.047886).  Saving model ...\n",
      "Epoch: 6 , train_loss: 0.07449  , train_accuracy: 66.71 , val_loss: 1.79176 , val_accuracy: {63.90}\n",
      "Validation loss decreased (120.047886 --> 120.047886).  Saving model ...\n",
      "Epoch: 7 , train_loss: 0.07449  , train_accuracy: 66.04 , val_loss: 1.79176 , val_accuracy: {64.88}\n",
      "Validation loss decreased (120.047886 --> 120.047886).  Saving model ...\n",
      "Epoch: 8 , train_loss: 0.07449  , train_accuracy: 66.11 , val_loss: 1.79176 , val_accuracy: {63.99}\n",
      "Validation loss decreased (120.047886 --> 120.047886).  Saving model ...\n",
      "Epoch: 9 , train_loss: 0.07449  , train_accuracy: 66.09 , val_loss: 1.79176 , val_accuracy: {64.32}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     80\u001b[0m loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 81\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     83\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mif device == \"cpu\":\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39m    optimizer.step()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m    clip.fine_tunning_model.convert_weights(fine_tunning_model)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m'''\u001b[39;00m    \n\u001b[1;32m     91\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m~/anaconda3/envs/sh_clip/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:67\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     66\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sh_clip/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sh_clip/lib/python3.8/site-packages/torch/optim/sgd.py:99\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     97\u001b[0m d_p \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mgrad\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 99\u001b[0m     d_p \u001b[39m=\u001b[39m d_p\u001b[39m.\u001b[39;49madd(p, alpha\u001b[39m=\u001b[39;49mweight_decay)\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m momentum \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    101\u001b[0m     param_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate[p]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_path = '/home/iai/Desktop/SH/cognex/model/'\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, fine_tunning_model.parameters()), lr=0.05, momentum=0.9, nesterov=True, weight_decay= 5 * 1e-2) \n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100)\n",
    "\n",
    "#optimizer = torch.optim.Adam(fine_tunning_model.parameters(), lr=0.00005)\n",
    "optimizer2 = torch.optim.Adam(zero_shot_model.parameters(), lr=0.00005)\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "#optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.05, momentum=0.9, nesterov=True, weight_decay=0.0005) \n",
    "#lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100)\n",
    "\n",
    "cross_entropy_loss_func = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(patience = 4, verbose = True, min_epoch=50)\n",
    "\n",
    "global_count = 0\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loss_arr =[]\n",
    "val_loss_arr =[]\n",
    "n = len(train_loader)\n",
    "\n",
    "label_list =['ok', 'scratch', 'fm', 'pin', 'dent', 'glue']\n",
    "context_list = ['repeat', 'lightness' , 'brigntness' , 'blur', 'cameraz']\n",
    "\n",
    "label_text = []\n",
    "for label in label_list:\n",
    "    label_text.append(\"a photo of \" + str(label))\n",
    "    \n",
    "context_label_text = []\n",
    "for label in label_list:\n",
    "    for context in context_list:\n",
    "        context_label_text.append(\"a \" +str(context) + \" of \" + str(label))\n",
    "\n",
    "label_text = clip.tokenize(label_text).to(device)\n",
    "context_label_text = clip.tokenize(context_label_text).to(device)\n",
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    \n",
    "    fine_tunning_model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_accuracy = 0.0\n",
    "    total = 0 \n",
    "       \n",
    "    for i,[image,label, context] in enumerate(train_loader):\n",
    "        \n",
    "        target = label.to(device)\n",
    "        input_image = image.to(device)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            zero_shot_image_features = zero_shot_model.encode_image(input_image).float()\n",
    "            zero_shot_context_text_features = zero_shot_model.encode_text(context_label_text).float()\n",
    "\n",
    "                \n",
    "            #zero_shot_image_features /= zero_shot_image_features.norm(dim=-1, keepdim=True)\n",
    "            #zero_shot_context_text_features /= zero_shot_context_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            zero_shot_output_context = (zero_shot_image_features @ zero_shot_context_text_features.T).softmax(dim=-1)\n",
    "\n",
    "        \n",
    "        fine_tunning_image_features = fine_tunning_model.encode_image(input_image).float()\n",
    "        fine_tunning_context_text_features = fine_tunning_model.encode_text(context_label_text).float()\n",
    "        \n",
    "        #fine_tunning_image_features /= fine_tunning_image_features.norm(dim=-1, keepdim=True)\n",
    "        #fine_tunning_context_text_features /= fine_tunning_context_text_features.norm(dim=-1, keepdim=True).requires_grad_(True)\n",
    "        fine_tunning_output_context = (fine_tunning_image_features @ fine_tunning_context_text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        \n",
    "        fine_tunning_label_text_features = fine_tunning_model.encode_text(label_text).float()\n",
    "        #fine_tunning_label_text_features /= fine_tunning_label_text_features.norm(dim=-1, keepdim=True).requires_grad_(True).float()\n",
    "        fine_tunning_output_label = (fine_tunning_image_features @ fine_tunning_label_text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        \n",
    "        loss = custom_loss(zero_shot_output_context, fine_tunning_output_context, fine_tunning_output_label, target, alpha=0.5)\n",
    "        \n",
    "        optimizer2.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        '''\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            #convert_models_to_fp32(fine_tunning_model)\n",
    "            optimizer.step()\n",
    "            clip.fine_tunning_model.convert_weights(fine_tunning_model)\n",
    "        '''    \n",
    "        torch.cuda.synchronize()\n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        total += target.size(0)\n",
    "        _, predicted = torch.max(fine_tunning_output_label, 1) \n",
    "        train_running_accuracy += (predicted ==target).sum().item() \n",
    "        \n",
    "        \n",
    "    train_loss_arr.append(train_running_loss / n)\n",
    "    train_accuracy = (100 * train_running_accuracy / total)\n",
    "    #lr_scheduler.step()      \n",
    "    \n",
    "    fine_tunning_model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_accuracy = 0.0\n",
    "    total = 0 \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for j,[image,label, context] in enumerate(val_loader):\n",
    "            \n",
    "            target = label.to(device)\n",
    "            input_image = image.to(device)\n",
    "            \n",
    "            fine_tunning_image_features = fine_tunning_model.encode_image(input_image).float()\n",
    "            fine_tunning_label_text_features = fine_tunning_model.encode_text(label_text).float()\n",
    "            fine_tunning_output_label = (fine_tunning_image_features @ fine_tunning_label_text_features.T).softmax(dim=-1)\n",
    "            \n",
    "            loss = cross_entropy_loss_func(fine_tunning_output_label,target)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(fine_tunning_output_label, 1) \n",
    "            val_running_accuracy += (predicted == target).sum().item() \n",
    "        \n",
    "        val_loss_arr.append(val_running_loss / len(val_loader))       \n",
    "        early_stopping(val_running_loss, fine_tunning_model)\n",
    "        torch.save(fine_tunning_model, model_path + str(epoch-1) + 'epoch_model.pt')\n",
    "        \n",
    "        val_accuracy = (100 * val_running_accuracy / total)  \n",
    "\n",
    "        print('Epoch: %d , train_loss: %.5f  , train_accuracy: %.2f , val_loss: %.5f , val_accuracy: {%.2f}' \n",
    "            %(epoch , train_running_loss / len(train_loader) ,train_accuracy , val_running_loss / len(val_loader) , val_accuracy))\n",
    "\n",
    "        \n",
    "        if early_stopping.early_stop: # 조건 만족 시 조기 종료\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(train_loss_arr,\n",
    "         color='skyblue',\n",
    "         marker='o', markerfacecolor='blue',\n",
    "         markersize=4)\n",
    "\n",
    "plt.plot(val_loss_arr,\n",
    "         color='pink',\n",
    "         marker='x', markerfacecolor='red',\n",
    "         markersize=4)\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class Customdataset(data.Dataset):\n",
    "    def __init__(self,transform = None):\n",
    "        dalist=None\n",
    "        self.file_paths = []\n",
    "        self.label = []\n",
    "        self.concept = []\n",
    "        self.transform = transform\n",
    "\n",
    "        file_path = '/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/imageset/single_image.2class.subject-split.trainval-repeat0/fold.5-5/ratio/100%/test.1.txt' # 폴더 경로\n",
    "\n",
    "        with open(file_path) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        image_names = [line.rstrip('\\n') for line in lines]\n",
    "\n",
    "\n",
    "        path = '/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/image' # 폴더 경로\n",
    "        files = os.listdir(path) # 해당 폴더에 있는 파일 이름을 리스트 형태로 받음\n",
    "        \n",
    "        with open('/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/annotation/single_image.6class.json', 'r') as json_file:\n",
    "            data = json.load(json_file)  \n",
    "    \n",
    "        for file in files:\n",
    "            if file in image_names:\n",
    "                self.label.append(data['single_image'][file]['class'][0])\n",
    "                self.file_paths.append(path + '/' + file)\n",
    "                \n",
    "                if str(file.split('_')[1]) == 'cameraz':\n",
    "                    self.concept.append(3)\n",
    "                    \n",
    "                elif str(file.split('_')[1]) == 'repeat':\n",
    "                    self.concept.append(0)\n",
    "                    \n",
    "                elif str(file.split('_')[1]) == 'brightness':\n",
    "                    self.concept.append(2)\n",
    "                    \n",
    "                elif str(file.split('_')[1]) == 'lcondition':\n",
    "                    self.concept.append(1)\n",
    "                    \n",
    "        print(len(label))\n",
    "        print(len(file_path))\n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.concept)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        label = self.label[idx]\n",
    "        concept = self.concept[idx]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, int(label)\n",
    "    \n",
    "    \n",
    "test_dataset = Customdataset(transform=data_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                               batch_size = 32                                       \n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tunning_model ,fine_tunning_preprocess= clip.load(\"RN50\", device=device)\n",
    "\n",
    "best_model_epoch = val_loss_arr.index(min(val_loss_arr))\n",
    "fine_tunning_model = torch.load(model_path + str(best_model_epoch) +'epoch_model.pt')\n",
    "fine_tunning_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "pred_ls = []\n",
    "real_ls = []\n",
    "\n",
    "context_ls = [] \n",
    "\n",
    "with torch.no_grad(): # 파라미터 업데이트 같은거 안하기 때문에 no_grad를 사용.\n",
    "  # net.eval() # batch normalization이나 dropout을 사용하지 않았기 때문에 사용하지 않음. 항상 주의해야함.\n",
    "  for data in test_loader:\n",
    "    target = label.to(device)\n",
    "    input_image = image.to(device)\n",
    "    \n",
    "    fine_tunning_image_features = fine_tunning_model.encode_image(input_image).float()\n",
    "    fine_tunning_label_text_features = fine_tunning_model.encode_text(label_text).float()\n",
    "    fine_tunning_output_label = (fine_tunning_image_features @ fine_tunning_label_text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    _, predicted = torch.max(fine_tunning_output_label, 1) \n",
    "    total += target.size(0) # test 개수\n",
    "    correct += (predicted == target).sum().item() # 예측값과 실제값이 맞으면 1 아니면 0으로 합산.\n",
    "  \n",
    "    pred_ls.append(predicted.cpu().numpy())\n",
    "    real_ls.append(target.cpu().numpy()) \n",
    "  \n",
    "\n",
    "print(f'accuracy of 1600 test images: {100*correct/total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(pred_ls).reshape(-1)\n",
    "real = np.array(real_ls).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(pred, real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(con_mat, labels, title='Confusion Matrix', cmap=plt.cm.get_cmap('Blues'), normalize=False):\n",
    "    plt.imshow(con_mat, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    marks = np.arange(len(labels))\n",
    "    nlabels = []\n",
    "    for k in range(len(con_mat)):\n",
    "        n = sum(con_mat[k])\n",
    "        nlabel = '{0}(n={1})'.format(labels[k],n)\n",
    "        nlabels.append(nlabel)\n",
    "    plt.xticks(marks, labels)\n",
    "    plt.yticks(marks, nlabels)\n",
    "\n",
    "    thresh = con_mat.max() / 2.\n",
    "    if normalize:\n",
    "        for i, j in itertools.product(range(con_mat.shape[0]), range(con_mat.shape[1])):\n",
    "            plt.text(j, i, '{0}%'.format(con_mat[i, j] * 100 / n), horizontalalignment=\"center\", color=\"white\" if con_mat[i, j] > thresh else \"black\")\n",
    "    else:\n",
    "        for i, j in itertools.product(range(con_mat.shape[0]), range(con_mat.shape[1])):\n",
    "            plt.text(j, i, con_mat[i, j], horizontalalignment=\"center\", color=\"white\" if con_mat[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "cm_df = confusion_matrix(pred, real)    \n",
    "label=['ok', 'scratch', 'fm', 'pin', 'dent', 'glue']\n",
    "plot_confusion_matrix(cm_df, labels=label, normalize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sh_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f483c09aa055d94f8a9399f471ed39cbc1f40cc0533ed05feeda2f72c4a44e43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
