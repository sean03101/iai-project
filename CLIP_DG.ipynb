{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "from cv2 import imread, normalize, resize\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import logging\n",
    "import sys\n",
    "from PIL import Image\n",
    "from pytorchtools import EarlyStopping # 위 링크의 깃허브 파일에서 임포트\n",
    "from utils import euclidean_metric, one_hot, count_acc\n",
    "from utils import pprint, set_gpu, ensure_path, AverageMeter, Timer, accuracy, one_hot\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torch import nn, optim, autograd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from random import sample, random\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset, TensorDataset, WeightedRandomSampler\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from random import sample, random\n",
    "\n",
    "import clip\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "batch_size = 32\n",
    "num_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customdataset(data.Dataset):\n",
    "    def __init__(self,transform = None):\n",
    "        dalist=None\n",
    "        self.file_paths = []\n",
    "        self.label = []\n",
    "        self.concept = []\n",
    "        self.transform = transform\n",
    "\n",
    "        folder_list = [\"custom_blur\",\"custom_brightness\",\"custom_cameraz\",\"custom_lightness\"]\n",
    "        \n",
    "        for folder_name in folder_list:\n",
    "            path_dir = \"/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/augmented_image2/train1-1/\" +  folder_name\n",
    "            file_list = os.listdir(path_dir)\n",
    "            \n",
    "            for file_name in file_list:\n",
    "                self.file_paths.append(path_dir + '/' + file_name)\n",
    "                self.label.append(file_name.split('_')[-1][0])\n",
    "                \n",
    "                if str(file_name.split('_')[2]) == 'cameraz':\n",
    "                    self.concept.append(3)\n",
    "                    \n",
    "                elif str(file_name.split('_')[2]) == 'blur':\n",
    "                    self.concept.append(3)\n",
    "                    \n",
    "                elif str(file_name.split('_')[2]) == 'brigntness':\n",
    "                    self.concept.append(2)\n",
    "                    \n",
    "                    \n",
    "                elif str(file_name.split('_')[2]) == 'lightness':\n",
    "                    self.concept.append(1)\n",
    "\n",
    "        \n",
    "        \n",
    "        file_path = '/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/imageset/single_image.2class.subject-split.trainval-repeat0/fold.5-5/ratio/100%/train.1-1.txt' # 폴더 경로\n",
    "\n",
    "        with open(file_path) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        image_names = [line.rstrip('\\n') for line in lines]\n",
    "        \n",
    "                \n",
    "        with open('/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/annotation/single_image.6class.json', 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            \n",
    "        path_image = \"/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/image/\"\n",
    "        files = os.listdir(path_image)\n",
    "        \n",
    "        for file in files:\n",
    "            if file in image_names:\n",
    "                self.label.append(data['single_image'][file]['class'][0])\n",
    "                self.file_paths.append(path_image + '/' + file)\n",
    "                self.concept.append(0)\n",
    "        \n",
    "        \n",
    "                \n",
    "        file_path = '/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/imageset/single_image.2class.subject-split.trainval-repeat0/fold.5-5/ratio/100%/validation.1-1.txt' # 폴더 경로\n",
    "\n",
    "        for folder_name in folder_list:\n",
    "            path_dir = \"/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/augmented_image2/train1-1/\" +  folder_name\n",
    "            file_list = os.listdir(path_dir)\n",
    "            \n",
    "            for file_name in file_list:\n",
    "                self.file_paths.append(path_dir + '/' + file_name)\n",
    "                self.label.append(file_name.split('_')[-1][0])\n",
    "                \n",
    "                if str(file_name.split('_')[2]) == 'cameraz':\n",
    "                    self.concept.append(3)\n",
    "                    \n",
    "                elif str(file_name.split('_')[2]) == 'blur':\n",
    "                    self.concept.append(3)\n",
    "                    \n",
    "                elif str(file_name.split('_')[2]) == 'brigntness':\n",
    "                    self.concept.append(2)\n",
    "                    \n",
    "                    \n",
    "                elif str(file_name.split('_')[2]) == 'lightness':\n",
    "                    self.concept.append(1)\n",
    "        \n",
    "        with open(file_path) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        image_names = [line.rstrip('\\n') for line in lines]\n",
    "        \n",
    "                \n",
    "        with open('/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/annotation/single_image.6class.json', 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            \n",
    "        path_image = \"/home/iai/Desktop/SH/cognex/d-sub-15pin_for-seoultech/image/\"\n",
    "        files = os.listdir(path_image)\n",
    "        \n",
    "        for file in files:\n",
    "            if file in image_names:\n",
    "                self.label.append(data['single_image'][file]['class'][0])\n",
    "                self.file_paths.append(path_image + '/' + file)\n",
    "                self.concept.append(0) \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        label = self.label[idx]\n",
    "        idx_num = idx\n",
    "        tmp_prob = np.random.rand()\n",
    "        weight_dict = {\"0\" :  (400/250) / (400/250 + 400 * 5 / 30), \"1\" : (400/30) / (400/250 + 400 * 5 / 30), \"2\" : (400/30) / (400/250 + 400 * 5 / 30), \n",
    "                       \"3\" : (400/30) / (400/250 + 400 * 5 / 30), \"4\" : (400/30) / (400/250 + 400 * 5 / 30), \"5\" : (400/30) / (400/250 + 400 * 5 / 30)}        \n",
    "        \n",
    "        if weight_dict[str(label)] > tmp_prob:\n",
    "            while True : \n",
    "                idx_num = np.random.randint(len(self.file_paths))\n",
    "                tmp_prob =  np.random.rand()\n",
    "                label = self.label[idx_num]\n",
    "                \n",
    "                if weight_dict[str(label)] < tmp_prob:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        path = self.file_paths[idx_num]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        concept = self.concept[idx_num]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, int(label), int(concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size :8576\n",
      "test_size :2144\n",
      "train_dataset :8576\n",
      "test dataset :2144\n"
     ]
    }
   ],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        \n",
    "        transforms.Resize(size=224),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "        ])\n",
    "\n",
    "##커스텀 데이터셋\n",
    "dataset = Customdataset(transform=data_transforms)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "print(f\"train size :{train_size}\")\n",
    "val_size = len(dataset) - train_size\n",
    "print(f\"test_size :{val_size}\")\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"train_dataset :{len(train_dataset)}\")\n",
    "print(f'test dataset :{len(val_dataset)}')\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size = batch_size, #배치사이즈\n",
    "                                               shuffle = True \n",
    "                                               )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                               batch_size = batch_size,                                              \n",
    "                                               shuffle = True \n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel, preprocess = clip.load(\"RN50\", device=device)\\nmodel.cuda().eval()\\ninput_resolution = model.visual.input_resolution\\ncontext_length = model.context_length\\nvocab_size = model.vocab_size\\n\\nprint(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\\nprint(\"Input resolution:\", input_resolution)\\nprint(\"Context length:\", context_length)\\nprint(\"Vocab size:\", vocab_size)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\"\"\"\n",
    "model, preprocess = clip.load(\"RN50\", device=device)\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_model, zero_shot_preprocess= clip.load(\"RN50\", device=device)\n",
    "\n",
    "# Freeze the model\n",
    "for param in zero_shot_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "fine_tunning_model ,fine_tunning_preprocess= clip.load(\"RN50\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=PIL.Image.BICUBIC)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7f89d0e26a60>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as distributions\n",
    "\n",
    "def custom_loss(zero_shot_output_context, fine_output_context, fine_output_label, target, alpha):\n",
    "    # Compute the cross entropy loss\n",
    "    ce_loss = F.cross_entropy(fine_output_label, target)\n",
    "    print(ce_loss)\n",
    "    # Compute the KL divergence between the output and the target\n",
    "    kl_loss = F.kl_div(zero_shot_output_context, fine_output_context)\n",
    "    print(ce_loss)\n",
    "    \n",
    "    # Return the mean of the cross entropy and KL divergence losses\n",
    "    return ce_loss + alpha * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 30])\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32, 6])\n",
      "torch.Size([32])\n",
      "tensor(1.7920, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7920, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.HalfTensor [6, 1024]], which is output 0 of MmBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 74\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m#ce_loss = F.cross_entropy(fine_tunning_output_label, target)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m#kl_loss = F.kl_div(zero_shot_output_context, fine_tunning_output_context)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m#loss = ce_loss + 1 * kl_loss\u001b[39;00m\n\u001b[1;32m     73\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 74\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     76\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     78\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m~/anaconda3/envs/sh_clip/lib/python3.8/site-packages/torch/tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Tensor \u001b[39mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[1;32m    214\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    215\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    216\u001b[0m         relevant_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         retain_graph\u001b[39m=\u001b[39mretain_graph,\n\u001b[1;32m    220\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph)\n\u001b[0;32m--> 221\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph)\n",
      "File \u001b[0;32m~/anaconda3/envs/sh_clip/lib/python3.8/site-packages/torch/autograd/__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 130\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    131\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph,\n\u001b[1;32m    132\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.HalfTensor [6, 1024]], which is output 0 of MmBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "model_path = '/home/iai/Desktop/SH/cognex/model/'\n",
    "optimizer = torch.optim.Adam(fine_tunning_model.parameters(), lr=0.00005)\n",
    "#optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.05, momentum=0.9, nesterov=True, weight_decay=0.0005) \n",
    "#lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 4, verbose = True, min_epoch=20)\n",
    "\n",
    "global_count = 0\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loss_arr =[]\n",
    "val_loss_arr =[]\n",
    "n = len(train_loader)\n",
    "\n",
    "label_list =['ok', 'scratch', 'fm', 'pin', 'dent', 'glue']\n",
    "context_list = ['repeat', 'lightness' , 'brigntness' , 'blur', 'cameraz']\n",
    "\n",
    "label_text = []\n",
    "for label in label_list:\n",
    "    label_text.append(\"a photo of \" + str(label))\n",
    "    \n",
    "context_label_text = []\n",
    "for label in label_list:\n",
    "    for context in context_list:\n",
    "        context_label_text.append(\"a \" +str(context) + \" of \" + str(label))\n",
    "\n",
    "label_text = clip.tokenize(label_text).to(device)\n",
    "context_label_text = clip.tokenize(context_label_text).to(device)\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    \n",
    "    #fine_tunning_model.train()\n",
    "    train_running_loss = 0.0\n",
    "       \n",
    "    for i,[image,label, context] in enumerate(train_loader):\n",
    "        \n",
    "        target = label.to(device)\n",
    "        input_image = image.to(device)\n",
    "        \n",
    "        zero_shot_image_features = zero_shot_model.encode_image(input_image)\n",
    "        zero_shot_context_text_features = zero_shot_model.encode_text(context_label_text)\n",
    "        \n",
    "        zero_shot_image_features /= zero_shot_image_features.norm(dim=-1, keepdim=True)\n",
    "        zero_shot_context_text_features /= zero_shot_context_text_features.norm(dim=-1, keepdim=True)\n",
    "        zero_shot_output_context = (zero_shot_image_features @ zero_shot_context_text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        \n",
    "        fine_tunning_image_features = fine_tunning_model.encode_image(input_image)\n",
    "        fine_tunning_context_text_features = fine_tunning_model.encode_text(context_label_text)\n",
    "        \n",
    "        fine_tunning_image_features /= fine_tunning_image_features.norm(dim=-1, keepdim=True)\n",
    "        fine_tunning_context_text_features /= fine_tunning_context_text_features.norm(dim=-1, keepdim=True)\n",
    "        fine_tunning_output_context = (fine_tunning_image_features @ fine_tunning_context_text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        \n",
    "        fine_tunning_label_text_features = fine_tunning_model.encode_text(label_text)\n",
    "        fine_tunning_label_text_features /= fine_tunning_label_text_features.norm(dim=-1, keepdim=True)\n",
    "        fine_tunning_output_label = (fine_tunning_image_features @ fine_tunning_label_text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        print(zero_shot_output_context.shape)\n",
    "        print(fine_tunning_output_context.shape)\n",
    "        print(fine_tunning_output_label.shape)\n",
    "        print(target.shape)\n",
    "        \n",
    "       \n",
    "        loss = custom_loss(zero_shot_output_context, fine_tunning_output_context, fine_tunning_output_label, target, alpha=1)\n",
    "        #ce_loss = F.cross_entropy(fine_tunning_output_label, target)\n",
    "        #kl_loss = F.kl_div(zero_shot_output_context, fine_tunning_output_context)\n",
    "        #loss = ce_loss + 1 * kl_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    train_loss_arr.append(train_running_loss / n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sh_clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f483c09aa055d94f8a9399f471ed39cbc1f40cc0533ed05feeda2f72c4a44e43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
